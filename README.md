# NLP Projects Repository

## Overview
This repository contains various Natural Language Processing (NLP) projects implemented as part of coursework at Brandeis Univeristy for 243COSI-114A-1 : Fundamentals of Natural Language Processing I. Each project explores different core NLP concepts, ranging from language modeling to part-of-speech tagging and classification tasks.

---

## **Projects Included**
### **1. N-Gram Probability Modeling**
- Implements **bigram and trigram models** to compute word sequence probabilities.
- Includes **Laplace smoothing** and **probability normalization**.
- Extracts **n-grams** from sentences and calculates their **probabilities**.

### **2. Smoothed N-Gram Language Modeling**
- Expands on n-gram models with **Maximum Likelihood Estimation (MLE)**.
- Implements **Lidstone smoothing** and **interpolation** for improved probability estimation.
- Evaluates models using **sequence probability** and **perplexity**.

### **3. Naive Bayes Classifier for Sentence Segmentation & Sentiment Analysis**
- Implements a **Naive Bayes classifier** for text classification.
- Classifies **sentence boundaries** and **airline tweet sentiment**.
- Uses **feature extraction** (unigrams, bigrams) and **Laplace smoothing**.

### **4. Hidden Markov Model (HMM) Part-of-Speech Tagger**
- Implements **Unigram, Bigram, and HMM-based POS tagging**.
- Uses **Greedy and Viterbi decoding** for tagging sequences.
- Computes **transition, emission, and initial probabilities**.

### **5. Multiclass Perceptron for Language Identification**
- Classifies text into **multiple languages** using **character n-grams**.
- Implements a **Perceptron with learning rate decay**.
- Uses **macro and weighted F1-score** for model evaluation.
- Includes **hyperparameter tuning** for best performance.

---

## **Usage**
Each project contains its own **README.md** with implementation details and examples of usage. To run a project, navigate to its directory and follow the instructions in its README.

---

## **Future Improvements**
- Experiment with **deep learning models** for POS tagging and text classification.
- Implement **additional smoothing techniques** for better language modeling.
- Extend classification models to **more languages and datasets**.

